{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *CoastSat*: example at Narrabeen-Collaroy, Australia\n",
    "\n",
    "This software is described in details in the following publications: \n",
    "- Shoreline detection:                      https://doi.org/10.1016/j.envsoft.2019.104528\n",
    "- Accuracy assessment and applications:     https://doi.org/10.1016/j.coastaleng.2019.04.004\n",
    "- Beach slope estimation:                   https://doi.org/10.1029/2020GL088365\n",
    "\n",
    "It enables the users to extract time-series of shoreline change over the last 30+ years at their site of interest.\n",
    "There are three main steps:\n",
    "1. Retrieval of the satellite images of the region of interest from Google Earth Engine\n",
    "2. Shoreline extraction at sub-pixel resolution\n",
    "3. Intersection of the shorelines with cross-shore transects\n",
    "4. Tidal correction \n",
    "\n",
    "## Initial settings\n",
    "\n",
    "Refer to the **Installation** section of the README for instructions on how to install the Python packages necessary to run the software, including Google Earth Engine Python API. If that step has been completed correctly, the following packages should be imported without any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "plt.ion()\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from coastsat import SDS_download, SDS_preprocess, SDS_shoreline, SDS_tools, SDS_transects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retrieval of the images from GEE\n",
    "\n",
    "Define the region of interest (`polygon`), the date range (`dates`) and the satellite missions (`sat_list`) from which you wish to retrieve the satellite images. The images will be cropped on the Google Earth Engine server and only the region of interest will be downloaded as a .tif file. The files will stored in the directory defined in `filepath`. \n",
    "\n",
    "Make sure the area of your ROI is smaller than 100 km2 (if larger split it into smaller ROIs).\n",
    "\n",
    "The function `SDS_download.check_images_available(inputs)` will print the number of images available for your inputs. The Landsat images are divided in Tier 1 and Tier 2, only Tier 1 images can be used for time-series analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images available between 2004-11-15 and 2009-01-30:\n",
      "- In Landsat Tier 1 & Sentinel-2 Level-1C:\n",
      "  L5: 114 images\n",
      "  L7: 159 images\n",
      "  L8: 0 images\n",
      "  S2: 0 images\n",
      "  Total: 273 images\n",
      "- In Landsat Tier 2:\n",
      "  L5: 62 images\n",
      "  L7: 66 images\n",
      "  L8: 0 images\n",
      "  Total: 128 images\n"
     ]
    }
   ],
   "source": [
    "# region of interest (longitude, latitude)\n",
    "polygon =  [[[-57.54774701943729, -38.04455906198859],\n",
    "          [-57.54774701943729, -38.08348307712578],\n",
    "          [-57.52800596108768, -38.08348307712578],\n",
    "          [-57.52800596108768, -38.04455906198859]]]\n",
    "\n",
    "\n",
    "  \n",
    "# date range\n",
    "dates = ['2004-11-15', '2009-01-30']\n",
    "# satellite missions\n",
    "sat_list = ['L5', 'L7','L8','S2']\n",
    "# name of the site\n",
    "sitename = 'MogotesTodo2004a2009'\n",
    "# directory where the data will be stored\n",
    "filepath = os.path.join(os.getcwd(), 'data')\n",
    "# put all the inputs into a dictionnary\n",
    "inputs = {'polygon': polygon, 'dates': dates, 'sat_list': sat_list, 'sitename': sitename, 'filepath':filepath}\n",
    "\n",
    "# before downloading the images, check how many images are available for your inputs\n",
    "SDS_download.check_images_available(inputs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `SDS_download.retrieve_images(inputs)` retrives the satellite images from Google Earth Engine.\n",
    "\n",
    "By default, only Landsat Tier 1 Top-of-Atmosphere and Sentinel-2 Level-1C products are downloaded. \n",
    "\n",
    "In case you need to access Tier 2 images for qualitative analysis, you need to set `inputs['include_T2'] = True` before calling `retrieve_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images available between 2004-11-15 and 2009-01-30:\n",
      "- In Landsat Tier 1 & Sentinel-2 Level-1C:\n",
      "  L5: 114 images\n",
      "  L7: 159 images\n",
      "  L8: 0 images\n",
      "  S2: 0 images\n",
      "  Total: 273 images\n",
      "- In Landsat Tier 2:\n",
      "  L5: 62 images\n",
      "  L7: 66 images\n",
      "  L8: 0 images\n",
      "  Total: 128 images\n",
      "\n",
      "Downloading images:\n",
      "L5: 114 images\n",
      "100%\n",
      "L7: 159 images\n",
      "100%\n",
      "L8: 0 images\n",
      "\n",
      "S2: 0 images\n",
      "\n",
      "0 out of 0 Sentinel-2 images were merged (overlapping or duplicate)\n"
     ]
    }
   ],
   "source": [
    "#inputs['include_T2'] = True\n",
    "metadata = SDS_download.retrieve_images(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you have already retrieved the images**, just load the metadata file by only running the section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = SDS_download.get_metadata(inputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shoreline extraction\n",
    "\n",
    "This section maps the position of the shoreline on the satellite images. The user can define the cloud threhold (`cloud_thresh`) and select the spatial reference system in which to output the coordinates of the mapped shorelines (`output_epsg`). See http://spatialreference.org/ to find the EPSG number corresponding to your local coordinate system. Make sure that your are using cartesian coordinates and not spherical coordinates (lat,lon) like WGS84. If unsure, use 3857 which is the web mercator projection (used by Google Maps).\n",
    "\n",
    "To quality control each shoreline detection and manually validate the mapped shorelines, the user has the option to set the parameter `check_detection` to **True**. To save a figure for each mapped shoreline set `save_figure` to **True**. \n",
    "\n",
    "The other parameters are for advanced users only and are described in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = { \n",
    "    # general parameters:\n",
    "    'cloud_thresh': 0.005,        # threshold on maximum cloud cover\n",
    "    'output_epsg': 32721,        # epsg code of spatial reference system desired for the output   \n",
    "    # quality control:\n",
    "    'check_detection': True,    # if True, shows each shoreline detection to the user for validation\n",
    "    'save_figure': False,        # if True, saves a figure showing the mapped shoreline for each image\n",
    "    # add the inputs defined previously\n",
    "    'inputs': inputs,\n",
    "    # [ONLY FOR ADVANCED USERS] shoreline detection parameters:\n",
    "    'min_beach_area': 1000,     # minimum area (in metres^2) for an object to be labelled as a beach\n",
    "    'buffer_size': 100,         # radius (in metres) of the buffer around sandy pixels considered in the shoreline detection\n",
    "    'min_length_sl': 2000,       # minimum length (in metres) of shoreline perimeter to be valid\n",
    "    'cloud_mask_issue': False,  # switch this parameter to True if sand pixels are masked (in black) on many images  \n",
    "    'sand_color': 'default',    # 'default', 'dark' (for grey/black sand beaches) or 'bright' (for white sand beaches)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Save .jpg of the satellite images \n",
    "Saves .jpg files of the preprocessed satellite images (cloud masking + pansharpening/down-sampling) under *./data/sitename/jpeg_files\\preprocessed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDS_preprocess.save_jpg(metadata, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Digitize a reference shoreline\n",
    "Creates a reference shoreline which helps to identify outliers and false detections. The reference shoreline is manually digitised by the user on one of the images. The parameter `max_dist_ref` defines the maximum distance from the reference shoreline (in metres) at which a valid detected shoreline can be. If you think that the default value of 100 m will not capture the full shoreline variability of your site, increase this value to an appropriate distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No cloud free images are available to digitise the reference shoreline,download more images and try again",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2533af6f9a58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'qt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reference_shoreline'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSDS_preprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reference_sl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_dist_ref'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;31m# max distance (in meters) allowed from the reference shoreline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CoastSat-master/coastsat/SDS_preprocess.py\u001b[0m in \u001b[0;36mget_reference_sl\u001b[0;34m(metadata, settings)\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;31m# check if a shoreline was digitised\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts_coords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m         raise Exception('No cloud free images are available to digitise the reference shoreline,'+\n\u001b[0m\u001b[1;32m    897\u001b[0m                         'download more images and try again') \n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: No cloud free images are available to digitise the reference shoreline,download more images and try again"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "settings['reference_shoreline'] = SDS_preprocess.get_reference_sl(metadata, settings)\n",
    "settings['max_dist_ref'] = 100 # max distance (in meters) allowed from the reference shoreline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch shoreline detection\n",
    "Extracts the 2D shorelines from the images in the spatial reference system specified by the user in `'output_epsg'`. The mapped shorelines are saved into `output.pkl` (under *./data/sitename*) and `output.geojson` (to be used in a GIS software).\n",
    "\n",
    "If you see that the sand pixels on the images are not being identified, change the parameter `sand_color` from `default` to `dark` or `bright` depending on the color of your beach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping shorelines:\n",
      "L5:   100%\n",
      "L7:   98%"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "output = SDS_shoreline.extract_shorelines(metadata, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then remove duplicates and images with inaccurate georeferencing (threhsold at 10m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = SDS_tools.remove_duplicates(output) # removes duplicates (images taken on the same date by the same satellite)\n",
    "output = SDS_tools.remove_inaccurate_georef(output, 10) # remove inaccurate georeferencing (set threshold to 10 m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple plot of the mapped shorelines. The coordinates are stored in the output dictionnary together with the exact dates in UTC time, the georeferencing accuracy and the cloud cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,8])\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Eastings')\n",
    "plt.ylabel('Northings')\n",
    "plt.grid(linestyle=':', color='0.5')\n",
    "for i in range(len(output['shorelines'])):\n",
    "    sl = output['shorelines'][i]\n",
    "    date = output['dates'][i]\n",
    "    plt.plot(sl[:,0], sl[:,1], '.', label=date.strftime('%d-%m-%Y'))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Shoreline analysis\n",
    "\n",
    "In this section we show how to compute time-series of cross-shore distance along user-defined shore-normal transects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you have already mapped the shorelines**, just load the output file (`output.pkl`) by running the section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(inputs['filepath'], sitename)\n",
    "with open(os.path.join(filepath, sitename + '_output' + '.pkl'), 'rb') as f:\n",
    "    output = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 options to define the coordinates of the shore-normal transects:\n",
    "\n",
    "**Option 1**: the user can interactively draw the shore-normal transects along the beach by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "transects = SDS_transects.draw_transects(output, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2**: the user can load the transect coordinates (make sure the spatial reference system is the same as defined previously by the parameter *output_epsg*) from a .geojson file by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_file = os.path.join(os.getcwd(), 'examples', 'NARRA_transects.geojson')\n",
    "transects = SDS_tools.transects_from_geojson(geojson_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3**: manually provide the coordinates of the transects as shown in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transects = dict([])\n",
    "transects['NA1'] = np.array([[-57.53600147909933, -38.0519985169024],[-57.532696997592986, -38.05304611597449]])\n",
    "transects['NA2'] = np.array([[-57.53949759448496, -38.0596427248181],[-57.53675101245371, -38.06058884516545]])\n",
    "transects['NA3'] = np.array([[-57.54299063917698, -38.06904732969389],[-57.539836361375464, -38.069334509176336]])\n",
    "transects['NA4'] = np.array([[-57.54299729907529, -38.07431283399702],[-57.54070132815854, -38.07407634942434]])\n",
    "transects['NA5'] = np.array([[-57.54176528655918, -38.07762985237507], [-57.53979118072422, -38.07722869093245]])\n",
    "transects['NA6'] = np.array([[-57.53649013519458, -38.08137465495203],[-57.534446291925235, -38.08161111592506]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the location of the transects, make sure they are in the right location with the origin always landwards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,8], tight_layout=True)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Eastings')\n",
    "plt.ylabel('Northings')\n",
    "plt.grid(linestyle=':', color='0.5')\n",
    "for i in range(len(output['shorelines'])):\n",
    "    sl = output['shorelines'][i]\n",
    "    date = output['dates'][i]\n",
    "    plt.plot(sl[:,0], sl[:,1], '.', label=date.strftime('%d-%m-%Y'))\n",
    "for i,key in enumerate(list(transects.keys())):\n",
    "    plt.plot(transects[key][0,0],transects[key][0,1], 'bo', ms=5)\n",
    "    plt.plot(transects[key][:,0],transects[key][:,1],'k-',lw=1)\n",
    "    plt.text(transects[key][0,0]-100, transects[key][0,1]+100, key,\n",
    "                va='center', ha='right', bbox=dict(boxstyle=\"square\", ec='k',fc='w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, intersect the transects with the 2D shorelines to obtain time-series of cross-shore distance.\n",
    "\n",
    "The time-series of shoreline change for each transect are saved in a .csv file in the data folder (all dates are in UTC time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the along-shore distance over which to consider shoreline points to compute the median intersection (robust to outliers)\n",
    "settings['along_dist'] = 25 \n",
    "cross_distance = SDS_transects.compute_intersection(output, transects, settings) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the time-series of shoreline change along each transect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,8], tight_layout=True)\n",
    "gs = gridspec.GridSpec(len(cross_distance),1)\n",
    "gs.update(left=0.05, right=0.95, bottom=0.05, top=0.95, hspace=0.05)\n",
    "for i,key in enumerate(cross_distance.keys()):\n",
    "    if np.all(np.isnan(cross_distance[key])):\n",
    "        continue\n",
    "    ax = fig.add_subplot(gs[i,0])\n",
    "    ax.grid(linestyle=':', color='0.5')\n",
    "    ax.set_ylim([-50,50])\n",
    "    ax.plot(output['dates'], cross_distance[key]- np.nanmedian(cross_distance[key]), '-o', ms=6, mfc='w')\n",
    "    ax.set_ylabel('distance [m]', fontsize=12)\n",
    "    ax.text(0.5,0.95, key, bbox=dict(boxstyle=\"square\", ec='k',fc='w'), ha='center',\n",
    "            va='top', transform=ax.transAxes, fontsize=14)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tidal correction\n",
    "\n",
    "This last section shows how to tidally-correct the time-series of shoreline change using time-series of tide level and an estimate of the beach slope.\n",
    "\n",
    "For this example, measured water levels for Sydney are stored in a csv file located [here](https://github.com/kvos/CoastSat/blob/master/examples/NARRA_tides.csv). When using your own file make sure that the dates are in UTC time, as the CoastSat shorelines are also in UTC, and the datum for the water levels is approx. Mean Sea Level.\n",
    "\n",
    "We assume that the beach slope at Narrabeen-Collaroy is 0.1 along all transects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the measured tide data\n",
    "filepath = os.path.join(os.getcwd(),'examples','Mogotes_tide_2004a2009.csv')\n",
    "tide_data = pd.read_csv(filepath, parse_dates=['dates'])\n",
    "dates_ts = [_.to_pydatetime() for _ in tide_data['dates']]\n",
    "tides_ts = np.array(tide_data['tide'])\n",
    "\n",
    "# get tide levels corresponding to the time of image acquisition\n",
    "dates_sat = output['dates']\n",
    "tides_sat = SDS_tools.get_closest_datapoint(dates_sat, dates_ts, tides_ts)\n",
    "\n",
    "# plot the subsampled tide data\n",
    "fig, ax = plt.subplots(1,1,figsize=(15,4), tight_layout=True)\n",
    "ax.grid(which='major', linestyle=':', color='0.5')\n",
    "ax.plot(tide_data['dates'], tide_data['tide'], '-', color='0.6', label='all time-series')\n",
    "ax.plot(dates_sat, tides_sat, '-o', color='k', ms=6, mfc='w',lw=1, label='image acquisition')\n",
    "ax.set(ylabel='tide level [m]',xlim=[dates_sat[0],dates_sat[-1]], title='Water levels at the time of image acquisition');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply tidal correction using a linear slope and a reference elevation to which project all the time-series of cross-shore change (to get time-series at Mean Sea Level, set `reference elevation` to 0. You also need an estimate of the beach slope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidal correction along each transect\n",
    "reference_elevation = 0 # elevation at which you would like the shoreline time-series to be\n",
    "beach_slope = 0.1\n",
    "cross_distance_tidally_corrected = {}\n",
    "for key in cross_distance.keys():\n",
    "    correction = (tides_sat-reference_elevation)/beach_slope\n",
    "    cross_distance_tidally_corrected[key] = cross_distance[key] + correction\n",
    "    \n",
    "# store the tidally-corrected time-series in a .csv file\n",
    "out_dict = dict([])\n",
    "out_dict['dates'] = dates_sat\n",
    "for key in cross_distance_tidally_corrected.keys():\n",
    "    out_dict['Transect '+ key] = cross_distance_tidally_corrected[key]\n",
    "df = pd.DataFrame(out_dict)\n",
    "fn = os.path.join(settings['inputs']['filepath'],settings['inputs']['sitename'],\n",
    "                  'transect_time_series_tidally_corrected.csv')\n",
    "df.to_csv(fn, sep=',')\n",
    "print('Tidally-corrected time-series of the shoreline change along the transects saved as:\\n%s'%fn)\n",
    "\n",
    "# plot the time-series of shoreline change (both raw and tidally-corrected)\n",
    "fig = plt.figure(figsize=[15,8], tight_layout=True)\n",
    "gs = gridspec.GridSpec(len(cross_distance),1)\n",
    "gs.update(left=0.05, right=0.95, bottom=0.05, top=0.95, hspace=0.05)\n",
    "for i,key in enumerate(cross_distance.keys()):\n",
    "    if np.all(np.isnan(cross_distance[key])):\n",
    "        continue\n",
    "    ax = fig.add_subplot(gs[i,0])\n",
    "    ax.grid(linestyle=':', color='0.5')\n",
    "    ax.set_ylim([-50,50])\n",
    "    ax.plot(output['dates'], cross_distance[key]- np.nanmedian(cross_distance[key]), '-o', ms=6, mfc='w', label='raw')\n",
    "    ax.plot(output['dates'], cross_distance_tidally_corrected[key]- np.nanmedian(cross_distance[key]), '-o', ms=6, mfc='w', label='tidally-corrected')\n",
    "    ax.set_ylabel('distance [m]', fontsize=12)\n",
    "    ax.text(0.5,0.95, key, bbox=dict(boxstyle=\"square\", ec='k',fc='w'), ha='center',\n",
    "            va='top', transform=ax.transAxes, fontsize=14)\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
